{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6dfe07a",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:matplotlib.pyplot:Loaded backend Agg version v2.2.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import logging\n",
    "import sys\n",
    "from contextlib import contextmanager\n",
    "from PIL import Image\n",
    "project_root = \"/home/heydari/FHHI-XAI/\"\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Assuming LCRP and src modules are available in the Python path or current directory\n",
    "# You might need to adjust sys.path if they are not.\n",
    "#import sys\n",
    "sys.path.append('/home/heydari/FHHI-XAI/')\n",
    "# sys.path.append('/path/to/your/src')\n",
    "\n",
    "from LCRP.models import get_model \n",
    "from src.plot_crp_explanations import plot_one_image_explanation, fig_to_array\n",
    "from src.plot_pcx_explanations_YOLO import plot_one_image_pcx_explanation\n",
    "from src.datasets.person_car_dataset import PersonCarDataset\n",
    "from src.datasets.flood_dataset import FloodDataset\n",
    "from src.plot_pcx_all import plot_pcx_explanations_pidnet\n",
    "from src.entities import get_person_vehicle_detection_explanation_entity, get_flood_segmentation_explanation_entity\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.helper import get_layer_names\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "# === CRP & Zennit ===\n",
    "import zennit.image as zimage\n",
    "from crp.image import imgify\n",
    "from crp.concepts import ChannelConcept\n",
    "from crp.helper import get_layer_names\n",
    "import gc\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "# Set non-interactive backend for matplotlib to avoid GUI issues in Flask\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import logging\n",
    "from contextlib import contextmanager\n",
    "\n",
    "from LCRP.models import get_model\n",
    "from src.plot_crp_explanations import plot_one_image_explanation, fig_to_array\n",
    "from src.plot_pcx_explanations_YOLO import plot_one_image_pcx_explanation\n",
    "from src.plotpcx_gpu import plot_pcx_explanations_pidnet\n",
    "from src.datasets.person_car_dataset import PersonCarDataset\n",
    "from src.datasets.flood_dataset import FloodDataset\n",
    "from src.entities import get_person_vehicle_detection_explanation_entity, get_flood_segmentation_explanation_entity\n",
    "from src.minio_client import FHHI_MINIO_BUCKET\n",
    "from src.memory_logging import log_cuda_memory\n",
    "# Mock FHHI_MINIO_BUCKET if minio_client is not fully set up for standalone execution\n",
    "try:\n",
    "    from src.minio_client import FHHI_MINIO_BUCKET\n",
    "except ImportError:\n",
    "    print(\"Warning: src.minio_client not found or not fully configured. Using a mock bucket name.\")\n",
    "    FHHI_MINIO_BUCKET = \"mock-bucket\"\n",
    "\n",
    "# Mock log_cuda_memory if not fully configured for standalone execution\n",
    "try:\n",
    "    from src.memory_logging import log_cuda_memory\n",
    "except ImportError:\n",
    "    print(\"Warning: src.memory_logging not found. Using a mock log_cuda_memory function.\")\n",
    "    def log_cuda_memory(logger, stage):\n",
    "        logger.info(f\"Mock CUDA memory log at stage: {stage}\")\n",
    "\n",
    "# Setup basic logging for demonstration\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e8c9d2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Project Root set to: /home/heydari/FHHI-XAI\n",
      "INFO:__main__:INIT CUDA Memory: 0.00GB allocated, 0.00GB max allocated, 0.00GB reserved, 23.65GB total\n",
      "INFO:__main__:Explanator initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "class Explanator:\n",
    "    \"\"\"Class that stores all loaded models together with all relevant data for generating CRP explanations.\n",
    "\n",
    "    This is the main class used in the TFA-02 component.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, project_root: str, logger: logging.Logger):\n",
    "        self.logger = logger\n",
    "        # General setup\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.dtype = torch.float32\n",
    "\n",
    "        # Log initial memory state\n",
    "        log_cuda_memory(self.logger, \"INIT\")\n",
    "\n",
    "        self.project_root = project_root\n",
    "\n",
    "        # Lazy loading approach - don't load models until needed\n",
    "        self._person_vehicle_model = None\n",
    "        self._person_car_dataset = None\n",
    "        self._flood_model = None\n",
    "        self._flood_dataset = None\n",
    "\n",
    "        # Create a mapping from entity types to handler methods\n",
    "        self.entity_handlers = {\n",
    "            \"BurntSegmentation\": self.explain_burnt_segmentation,\n",
    "            \"FireSegmentation\": self.explain_fire_segmentation,\n",
    "            \"FloodSegmentation\": self.explain_flood_segmentation,\n",
    "            \"PersonVehicleDetection\": self.explain_person_vehicle_detection,\n",
    "            \"SmokeSegmentation\": self.explain_smoke_segmentation,\n",
    "            \"EOBurntArea\": self.explain_eo_burnt_area,\n",
    "            \"EOFloodExtent\": self.explain_eo_flood_extent,\n",
    "            \"ImageMetadata\": None,\n",
    "        }\n",
    "\n",
    "        self.VALID_ENTITY_TYPES = list(self.entity_handlers.keys())\n",
    "        self.DLR_ENTITY_TYPES = {\"EOBurntArea\", \"EOFloodExtent\"}\n",
    "\n",
    "        self.running_avg_forward_time = 0\n",
    "        self.forward_count = 0\n",
    "        self.running_avg_backward_time = 0\n",
    "        self.backward_count = 0\n",
    "\n",
    "    @property\n",
    "    def prediction_times(self):\n",
    "        \"\"\"Returns the average forward and backward pass times.\"\"\"\n",
    "        return {\n",
    "            \"forward\": f\"{self.running_avg_forward_time:.3f} ms\",\n",
    "            \"backward\": f\"{self.running_avg_backward_time:.3f} ms\",\n",
    "        }\n",
    "\n",
    "    @contextmanager\n",
    "    def record_forward_time(self):\n",
    "        \"\"\"Context manager to record the time taken for a forward pass.\"\"\"\n",
    "        if self.device == \"cuda\" and torch.cuda.is_available():\n",
    "            self.logger.debug(\"Using CUDA for timing\")\n",
    "            try:\n",
    "                # Try using CUDA events for timing on GPU\n",
    "                start_time = torch.cuda.Event(enable_timing=True)\n",
    "                end_time = torch.cuda.Event(enable_timing=True)\n",
    "                start_time.record()\n",
    "                yield\n",
    "                end_time.record()\n",
    "                # Wait for the events to be recorded\n",
    "                torch.cuda.synchronize()\n",
    "                elapsed_time = start_time.elapsed_time(end_time)\n",
    "            except (TypeError, RuntimeError):\n",
    "                # Fall back to time.time() if CUDA events fail\n",
    "                import time\n",
    "                start_time = time.time()\n",
    "                yield\n",
    "                elapsed_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "        else:\n",
    "            self.logger.debug(\"Using CPU for timing\")\n",
    "            print()\n",
    "            # Use time.time() for timing on CPU\n",
    "            import time\n",
    "            start_time = time.time()\n",
    "            yield\n",
    "            elapsed_time = (time.time() - start_time) * 1000  # Convert to milliseconds\n",
    "\n",
    "        # The formula for the running average is:\n",
    "        # new_average = old_average + (new_value - old_average) / new_count\n",
    "        self.forward_count += 1\n",
    "        self.running_avg_forward_time += (elapsed_time - self.running_avg_forward_time) / self.forward_count\n",
    "        self.logger.debug(f\"Forward pass time: {elapsed_time:.2f} ms\")\n",
    "        self.logger.debug(f\"Running average forward pass time: {self.running_avg_forward_time:.2f} ms\")\n",
    "\n",
    "    def explain(self, entity_type: str, original_image_bucket: str, original_image_filename: str, image: np.ndarray, bm_id, alert_ref):\n",
    "        \"\"\"Generate explanation for the given entity type and image.\"\"\"\n",
    "        log_cuda_memory(self.logger, f\"BEFORE EXPLAIN {entity_type}\")\n",
    "\n",
    "        if entity_type not in self.VALID_ENTITY_TYPES:\n",
    "            raise ValueError(f\"Invalid entity type: {entity_type}. Must be one of {self.VALID_ENTITY_TYPES}\")\n",
    "\n",
    "        # Get the appropriate handler method for this entity type\n",
    "        handler = self.entity_handlers.get(entity_type)\n",
    "\n",
    "        # Call the handler method with the image\n",
    "        result = handler(original_image_bucket, original_image_filename, image, bm_id=bm_id, alert_ref=alert_ref)\n",
    "\n",
    "        log_cuda_memory(self.logger, f\"AFTER EXPLAIN {entity_type}\")\n",
    "        # Clear unnecessary tensors from cache\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return result\n",
    "\n",
    "    def explain_eo_burnt_area(self, original_image_bucket: str, original_image_filename: str, image: np.ndarray):\n",
    "        raise NotImplementedError(\"EO Burnt Area explanation is not implemented yet.\")\n",
    "\n",
    "    def explain_eo_flood_extent(self, original_image_bucket: str, original_image_filename: str, image: np.ndarray):\n",
    "        raise NotImplementedError(\"EO Flood Extent explanation is not implemented yet.\")\n",
    "\n",
    "    def explain_burnt_segmentation(self, original_image_bucket: str, original_image_filename: str, image: np.ndarray):\n",
    "        raise NotImplementedError(\"Burnt segmentation explanation is not implemented yet.\")\n",
    "\n",
    "    def explain_fire_segmentation(self, original_image_bucket: str, original_image_filename: str, image: np.ndarray):\n",
    "        raise NotImplementedError(\"Fire segmentation explanation is not implemented yet.\")\n",
    "\n",
    "    @property\n",
    "    def flood_model(self):\n",
    "        if self._flood_model is None:\n",
    "            log_cuda_memory(self.logger, \"BEFORE LOADING FLOOD MODEL\")\n",
    "            model_name = \"pidnet\"\n",
    "            # flood_model_path = os.path.join(self.project_root, \"models\", \"flood_s_best_pidnet_modified.pt\")\n",
    "            self._flood_model = get_model(model_name=model_name)\n",
    "            log_cuda_memory(self.logger, \"AFTER LOADING FLOOD MODEL\")\n",
    "        return self._flood_model\n",
    "\n",
    "    @property\n",
    "    def flood_dataset(self):\n",
    "        if self._flood_dataset is None:\n",
    "            flood_data_path = os.path.join(self.project_root, \"data\", \"General_Flood_v3\")\n",
    "\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Lambda(lambda x: x.half()),\n",
    "            ])\n",
    "\n",
    "            self._flood_dataset = FloodDataset(root_dir=flood_data_path, split=\"train\", transform=transform)\n",
    "        return self._flood_dataset\n",
    "\n",
    "    def explain_flood_segmentation(self, original_image_bucket: str, original_image_filename: str, image: np.ndarray, bm_id, alert_ref):\n",
    "        \"\"\"Generate flood segmentation explanation using PCX.\"\"\"\n",
    "        log_cuda_memory(self.logger, \"FLOOD_SEG START\")\n",
    "\n",
    "        # Parameters\n",
    "        class_id = 1  # Flood class ID\n",
    "        n_concepts = 3\n",
    "        n_refimgs = 12\n",
    "        model_name = \"pidnet\"\n",
    "        num_prototypes = 2\n",
    "        output_dir_pcx = \"output/pcx/pidnet_flood/\"\n",
    "        output_dir_crp = \"output/crp/pidnet_flood/\"\n",
    "        ref_imgs_path = \"output/ref_imgs_pidnet/\"\n",
    "        # layer_names = get_layer_names(self.flood_model, [torch.nn.Conv2d])\n",
    "        layer_name = 'layer5.0.conv1'\n",
    "        print(layer_name)\n",
    "        # Apply transform to the input test image\n",
    "        log_cuda_memory(self.logger, \"BEFORE IMAGE TRANSFORM\")\n",
    "        image_tensor = self.flood_dataset.transform(image)\n",
    "        image_tensor = image_tensor.to(self.device, non_blocking=True)\n",
    "\n",
    "        log_cuda_memory(self.logger, \"AFTER IMAGE TRANSFORM\")\n",
    "\n",
    "        print(\"Shape after batch dimension:\", image_tensor.shape)\n",
    "\n",
    "        log_cuda_memory(self.logger, \"BEFORE EXPLANATION GENERATION\")\n",
    "        try:\n",
    "            explanation_fig = plot_pcx_explanations_pidnet(\n",
    "                model_name,\n",
    "                self.flood_model,\n",
    "                self.flood_dataset,\n",
    "                image_tensor=image_tensor,\n",
    "                layer_name=layer_name,\n",
    "                n_concepts=n_concepts,\n",
    "                n_refimgs=n_refimgs,\n",
    "                num_prototypes=num_prototypes,\n",
    "                ref_imgs_path=ref_imgs_path,\n",
    "                output_dir_crp=output_dir_crp,\n",
    "                output_dir_pcx=output_dir_pcx,\n",
    "                precision=\"autocast_fp16\",\n",
    "            )\n",
    "        finally:\n",
    "            # Release the input tensor as soon as the attribution run finishes\n",
    "            del image_tensor\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        # fig is returned implicitly as part of this function; adapt if needed\n",
    "        log_cuda_memory(self.logger, \"AFTER EXPLANATION GENERATION\")\n",
    "\n",
    "        explanation_img = fig_to_array(explanation_fig)\n",
    "        plt.close(explanation_fig)\n",
    "        gc.collect()\n",
    "\n",
    "        # Prepare explanation entity\n",
    "        original_entity_type = \"FloodSegmentation\"\n",
    "        explanation_image_filename = f\"tfa02/{original_entity_type}/{original_image_filename}\"\n",
    "\n",
    "        explanation_entity = get_flood_segmentation_explanation_entity(\n",
    "            original_image_bucket=original_image_bucket,\n",
    "            original_image_filename=original_image_filename,\n",
    "            explanation_image_bucket=FHHI_MINIO_BUCKET,\n",
    "            explanation_image_filename=explanation_image_filename,\n",
    "            class_id=class_id,\n",
    "            n_concepts=n_concepts,\n",
    "            n_refimgs=n_refimgs,\n",
    "            layer=layer_name,\n",
    "            mode=\"relevance\",\n",
    "            bm_id=bm_id,\n",
    "            alert_ref=alert_ref\n",
    "        )\n",
    "\n",
    "        log_cuda_memory(self.logger, \"FLOOD_SEG END\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return explanation_entity, [explanation_img], [explanation_image_filename]\n",
    "\n",
    "    @property\n",
    "    def person_vehicle_model(self):\n",
    "        if self._person_vehicle_model is None:\n",
    "            log_cuda_memory(self.logger, \"BEFORE LOADING PERSON VEHICLE MODEL\")\n",
    "            self._person_vehicle_model = self.load_person_vehicle_model()\n",
    "            log_cuda_memory(self.logger, \"AFTER LOADING PERSON VEHICLE MODEL\")\n",
    "        return self._person_vehicle_model\n",
    "\n",
    "    @property\n",
    "    def person_car_dataset(self):\n",
    "        if self._person_car_dataset is None:\n",
    "            self._person_car_dataset = self.load_person_car_data()\n",
    "        return self._person_car_dataset\n",
    "\n",
    "    def load_person_vehicle_model(self):\n",
    "        # Load the person/vehicle detection model\n",
    "        model_name = \"yolov6s6\"\n",
    "        person_vehicle_model_path = os.path.join(self.project_root, \"models\", \"yolo_person_car_detection_ckpt.pt\")\n",
    "        return get_model(model_name=model_name, classes=2, ckpt_path=person_vehicle_model_path, device=self.device,\n",
    "                         dtype=self.dtype)\n",
    "\n",
    "    def load_person_car_data(self):\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),  # Convert to tensor\n",
    "            transforms.Resize((640, 640)),\n",
    "            transforms.Lambda(lambda x: x.to(self.dtype)),\n",
    "        ])\n",
    "\n",
    "        person_car_data_path = os.path.join(self.project_root, \"data\", \"person_car_detection_data\", \"Arthal\")\n",
    "        dataset = PersonCarDataset(root_dir=person_car_data_path, split=\"train\", transform=transform)\n",
    "        return dataset\n",
    "    def explain_person_vehicle_detection(self, original_image_bucket: str, original_image_filename: str,\n",
    "                                         image: np.ndarray, bm_id, alert_ref):\n",
    "        \"\"\"Generate person/vehicle detection explanation.\"\"\"\n",
    "        original_entity_type = \"PersonVehicleDetection\"\n",
    "        original_filename_no_ext = os.path.splitext(original_image_filename)[0]\n",
    "\n",
    "        log_cuda_memory(self.logger, \"PERSON_VEHICLE START\")\n",
    "\n",
    "        model_name = \"yolov6s6\"\n",
    "        n_concepts = 3\n",
    "        n_refimgs = 12\n",
    "        # This one was used before for CRP\n",
    "        # layer = \"module.backbone.ERBlock_6.2.cspsppf.cv7.block.conv\"\n",
    "        # This one suggested by Jawher for PCX\n",
    "        layer = 'module.backbone.ERBlock_3.0.rbr_dense.conv'\n",
    "        prototype_dict = {0: 4, 1: 5}\n",
    "\n",
    "        mode = \"relevance\"\n",
    "\n",
    "        crp_output_dir = \"output/crp/yolo_person_car\"\n",
    "        pcx_output_dir = \"output/pcx/yolo_person_car\"\n",
    "        ref_imgs_path = \"output/ref_imgs_12\"\n",
    "\n",
    "        # Apply transform\n",
    "        log_cuda_memory(self.logger, \"BEFORE IMAGE TRANSFORM\")\n",
    "        image_tensor = self.person_car_dataset.transform(image)\n",
    "        log_cuda_memory(self.logger, \"AFTER IMAGE TRANSFORM\")\n",
    "\n",
    "        # We need to run the model to get the predicted boxes\n",
    "        test_img = self.person_car_dataset.transform(image)\n",
    "        test_img = test_img.unsqueeze(0)\n",
    "\n",
    "        test_img = test_img.to(self.device)\n",
    "\n",
    "        with self.record_forward_time():\n",
    "            scores, boxes = self.person_vehicle_model.predict_with_boxes(test_img)\n",
    "        num_boxes = boxes.shape[1]\n",
    "        self.logger.debug(f\"Number of boxes: {num_boxes}\")\n",
    "\n",
    "        # Only for debug\n",
    "        # self.logger.warning(f\"Changing num_boxes from {num_boxes} to 2 for debug\")\n",
    "        # num_boxes = 2\n",
    "\n",
    "        boxes = boxes[0].cpu().detach().numpy().tolist()\n",
    "\n",
    "        class_ids = scores[0].argmax(dim=1)\n",
    "        confidences = scores[0].max(dim=1).values\n",
    "\n",
    "        explanation_images = []\n",
    "        explanation_image_filenames = []\n",
    "\n",
    "        explanation_boxes = []\n",
    "        for prediction_num in range(num_boxes):\n",
    "            exp_box = {}\n",
    "\n",
    "            exp_box[\"object_id\"] = prediction_num\n",
    "            exp_box[\"bbox\"] = boxes[prediction_num]\n",
    "            class_id = class_ids[prediction_num].item()\n",
    "            confidence = confidences[prediction_num].item()\n",
    "            exp_box[\"class_id\"] = class_id\n",
    "            exp_box[\"confidences\"] = confidence\n",
    "\n",
    "            self.logger.debug(f\"Generating explanation for box {prediction_num} of {num_boxes}\")\n",
    "            log_cuda_memory(self.logger, f\"BEFORE BOX {prediction_num}\")\n",
    "\n",
    "            # Clear cache before each box processing\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # CRP visualization\n",
    "            # explanation_fig = plot_one_image_explanation(\n",
    "            #     model_name, self.person_vehicle_model, image_tensor,\n",
    "            #     self.person_car_dataset, class_id, layer, prediction_num,\n",
    "            #     mode, n_concepts, n_refimgs, output_dir=glocal_analysis_output_dir\n",
    "            # )\n",
    "\n",
    "            # PCX visualization\n",
    "            explanation_fig = plot_one_image_pcx_explanation(\n",
    "                model_name, self.person_vehicle_model, image_tensor,\n",
    "                self.person_car_dataset, class_id, n_concepts, n_refimgs,\n",
    "                num_prototypes=prototype_dict,\n",
    "                prediction_num=prediction_num,\n",
    "                layer_name=layer,\n",
    "                ref_imgs_path=ref_imgs_path,\n",
    "                output_dir_pcx=pcx_output_dir,\n",
    "                output_dir_crp=crp_output_dir,\n",
    "                outside_logger=self.logger,\n",
    "            )\n",
    "\n",
    "            explanation_img = fig_to_array(explanation_fig)\n",
    "            explanation_images.append(explanation_img)\n",
    "\n",
    "            explanation_file_name = f\"tfa02/{original_entity_type}/{original_filename_no_ext}/object_{prediction_num}.png\"\n",
    "            explanation_image_filenames.append(explanation_file_name)\n",
    "\n",
    "            exp_box[\"explanation_image\"] = explanation_file_name\n",
    "            exp_box[\"explanation_image_bucket\"] = FHHI_MINIO_BUCKET\n",
    "\n",
    "            log_cuda_memory(self.logger, f\"AFTER BOX {prediction_num}\")\n",
    "\n",
    "            # Force garbage collection after each box\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            explanation_boxes.append(exp_box)\n",
    "\n",
    "        explanation_entity = get_person_vehicle_detection_explanation_entity(\n",
    "            original_image_bucket=original_image_bucket,\n",
    "            original_image_filename=original_image_filename,\n",
    "            original_detection_boxes=boxes,\n",
    "            original_detection_class_categories=class_ids.cpu().detach().numpy().tolist(),\n",
    "            original_detection_confidences=confidences.cpu().detach().numpy().tolist(),\n",
    "            explanation_boxes=explanation_boxes,\n",
    "            n_concepts=n_concepts,\n",
    "            n_refimgs=n_refimgs,\n",
    "            layer=layer,\n",
    "            mode=mode,\n",
    "            bm_id=bm_id,\n",
    "            alert_ref=alert_ref\n",
    "        )\n",
    "        self.logger.warning(f\"explanation_entity: {explanation_entity}\")\n",
    "\n",
    "        log_cuda_memory(self.logger, \"PERSON_VEHICLE END\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return explanation_entity, explanation_images, explanation_image_filenames\n",
    "\n",
    "    def explain_smoke_segmentation(self, src_entity: dict, image: np.ndarray):\n",
    "        raise NotImplementedError(\"Smoke segmentation explanation is not implemented yet.\")\n",
    "\n",
    "\n",
    "# Configure basic logging if not done elsewhere\n",
    "if not logging.getLogger().handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set your project root directory.\n",
    "# Adjust this path to where your 'models', 'data', 'LCRP', and 'src' directories are located.\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "logger.info(f\"Project Root set to: {project_root}\")\n",
    "\n",
    "# THIS IS WHERE THE 'explanator' OBJECT IS CREATED\n",
    "explanator = Explanator(project_root=project_root, logger=logger)\n",
    "logger.info(\"Explanator initialized successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c8ac109",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Project Root set to: /home/heydari/FHHI-XAI\n",
      "INFO:__main__:INIT CUDA Memory: 0.00GB allocated, 0.00GB max allocated, 0.00GB reserved, 23.65GB total\n",
      "INFO:__main__:Explanator initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging # Make sure logging is imported if not already\n",
    "\n",
    "# Configure basic logging if not done elsewhere\n",
    "if not logging.getLogger().handlers:\n",
    "    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Set your project root directory.\n",
    "# Adjust this path to where your 'models', 'data', 'LCRP', and 'src' directories are located.\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "logger.info(f\"Project Root set to: {project_root}\")\n",
    "\n",
    "# THIS IS WHERE THE 'explanator' OBJECT IS CREATED\n",
    "explanator = Explanator(project_root=project_root, logger=logger)\n",
    "logger.info(\"Explanator initialized successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50feb4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:BEFORE EXPLAIN FloodSegmentation CUDA Memory: 0.00GB allocated, 0.00GB max allocated, 0.00GB reserved, 23.65GB total\n",
      "INFO:__main__:FLOOD_SEG START CUDA Memory: 0.00GB allocated, 0.00GB max allocated, 0.00GB reserved, 23.65GB total\n",
      "INFO:__main__:BEFORE IMAGE TRANSFORM CUDA Memory: 0.00GB allocated, 0.00GB max allocated, 0.00GB reserved, 23.65GB total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Explaining Flood Segmentation ---\n",
      "Could not load or create image for flood explanation: 'str' object has no attribute 'plot'. Creating a generic dummy image.\n",
      "layer5.0.conv1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:AFTER IMAGE TRANSFORM CUDA Memory: 0.00GB allocated, 0.00GB max allocated, 0.02GB reserved, 23.65GB total\n",
      "INFO:__main__:BEFORE EXPLANATION GENERATION CUDA Memory: 0.00GB allocated, 0.00GB max allocated, 0.02GB reserved, 23.65GB total\n",
      "INFO:__main__:BEFORE LOADING FLOOD MODEL CUDA Memory: 0.00GB allocated, 0.00GB max allocated, 0.02GB reserved, 23.65GB total\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape after batch dimension: torch.Size([3, 512, 512])\n",
      "Error during Flood Segmentation explanation: get_pidnet() missing 1 required positional argument: 'model_name'\n"
     ]
    }
   ],
   "source": [
    "# Example for Flood Segmentation\n",
    "print(\"\\n--- Explaining Flood Segmentation ---\")\n",
    "original_image_bucket_flood = \"data/General_Flood_v3/RGB/train/JPEG\"\n",
    "original_image_filename_flood = \"image_0.jpg\"\n",
    "\n",
    "# Create a dummy image for demonstration if you don't have one\n",
    "try:\n",
    "    # Attempt to load a real image (adjust path as needed)\n",
    "    # This path is a common structure for datasets, e.g., 'data/General_Flood_v3/images/train/some_image.jpg'\n",
    "    #sample_image_path_flood = os.path.join(project_root, \"data\", \"General_Flood_v3\", \"RGB\", \"train\", \"JPEG\",original_image_filename_flood) \n",
    "\n",
    "    sample_image_path_flood = 'data/General_Flood_v3/RGB/train/JPEG/image_0.jpg'\n",
    "    sample_image_path_flood.plot()\n",
    "    if not os.path.exists(sample_image_path_flood):\n",
    "        # Fallback to a generated dummy image if real one not found\n",
    "        print(f\"Sample flood image not found at {sample_image_path_flood}. Creating a dummy image.\")\n",
    "        dummy_image_flood = np.random.randint(20, 256, (512, 512, 3), dtype=np.uint8) # Common size for segmentation\n",
    "        Image.fromarray(dummy_image_flood).save(original_image_filename_flood) # Save to current notebook directory\n",
    "        image_flood = Image.open(original_image_filename_flood).convert('RGB')\n",
    "    else:\n",
    "        image_flood = Image.open(sample_image_path_flood).convert('RGB')\n",
    "    image_flood_np = np.array(image_flood)\n",
    "except Exception as e:\n",
    "    print(f\"Could not load or create image for flood explanation: {e}. Creating a generic dummy image.\")\n",
    "    image_flood_np = np.random.randint(0, 256, (512, 512, 3), dtype=np.uint8) # Default dummy if all fails\n",
    "\n",
    "try:\n",
    "    flood_explanation_entity, flood_explanation_imgs, flood_explanation_filenames = \\\n",
    "        explanator.explain(\n",
    "            entity_type=\"FloodSegmentation\", \n",
    "            original_image_bucket=original_image_bucket_flood, \n",
    "            original_image_filename=original_image_filename_flood, \n",
    "            image=image_flood_np,\n",
    "            bm_id='bm_001',\n",
    "            alert_ref=\"alert_12345\"\n",
    "        )\n",
    "    \n",
    "    print(\"Flood Explanation Entity:\")\n",
    "    print(flood_explanation_entity)\n",
    "    \n",
    "    print(\"Flood Explanation Images (first one displayed):\")\n",
    "    if flood_explanation_imgs:\n",
    "        plt.figure() # Adjust figure size for better display\n",
    "        plt.imshow(flood_explanation_imgs[0])\n",
    "        plt.title(f\"Flood Explanation: {flood_explanation_filenames[0].split('/')[-1]}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "except NotImplementedError as e:\n",
    "    print(f\"Skipping Flood Segmentation explanation: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during Flood Segmentation explanation: {e}\")\n",
    "    # If model/data loading fails, you might get more specific errors here.\n",
    "    # Ensure your project_root is correctly set and models/data are present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12e70a4",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "v-310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
